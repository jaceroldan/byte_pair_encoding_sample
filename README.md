# Byte Pair Encoding Algorithm

Goal: Create a text tokenizer using a bottom-up approach as discussed in class. Use the dataset found in the Kaggle notebook in the lecture. Limit yourselves to a random sample of 1000 documents. For the sentence segmentation, use any algorithm, cite your references.

main:
https://www.kaggle.com/competitions/coleridgeinitiative-show-us-the-data/data
alternate (you didn't get it from here):
https://www.dropbox.com/scl/fi/ow2o40dpm8tu2ahg3muet/coleridgeinitiative-show-us-the-data.zip?rlkey=c5ffx9iol8cdkyymumtalbt9w&dl=0

Compare your tokenizer with the Wordpiece tokenizer found in the Hugging Face transformer library. Identify the number of tokens in each, where did the difference come from?
